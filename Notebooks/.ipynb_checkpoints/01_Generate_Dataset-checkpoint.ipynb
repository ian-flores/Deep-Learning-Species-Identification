{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Alejandro Vega & Ian Flores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the necessary dependencies\n",
    "The installation of this dependencies and the Python version (3.6) here used is better suited if doing with Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import wave\n",
    "import openpyxl\n",
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "import _pickle as cpl\n",
    "import tarfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have .flac files in our dataset, we need to convert this to .wav format before doing the transformations to spectrograms. To be able to do this, we call bash script, specifically the 'sox' package to help us. For this, you have to have 'sox' installed in your computer. In Ubuntu 17.10, the command is pretty straightforward. 'sudo apt install sox'. After the transformation we store all the files in format .wav in a directory called 'wav_recordings'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the recordings and separate them by format. \n",
    "flac_files = []\n",
    "wav_files = []\n",
    "for file in os.listdir(\"../dataset/recordings\"):\n",
    "    if file.endswith(\".flac\"):\n",
    "        flac_files.append(file)\n",
    "    elif file.endswith(\".wav\"):\n",
    "        wav_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert all the .flac files to .wav files and store them in 'wav_recordings'\n",
    "for i in range(len(flac_files)):\n",
    "    string = 'sox ../recordings/' + str(flac_files[i]) + ' ../wav_recordings/' + str(flac_files[i][:-5]) + '.wav'\n",
    "    os.system(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store the .wav files in 'wav_recordings'\n",
    "for i in range(len(wav_files)):\n",
    "    string = 'mv ../' + str(wav_files[i]) + ' ../wav_recordings'\n",
    "    os.system(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations and Spectrogram Informations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we first get the information from the wav files, then the information regarding the spectrogram, and it's Region of Interest, which is where the animal call is focused. Then, after all this information is collected we proceed to plot the spectrogram for the full recording. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the Info from the .wav file.\n",
    "def wavInfo(rec_file):\n",
    "    wav_file = wave.open(rec_file, 'r')\n",
    "    frames = wav_file.readframes(-1)\n",
    "    wave_info = pylab.fromstring(frames, 'Int16') #all .wavs in our dataset are 16bit\n",
    "    framerate = wav_file.getframerate()\n",
    "    wav_file.close()\n",
    "    return wave_info, framerate\n",
    "\n",
    "# Get the info from the Spectrogram, but don't plot it.\n",
    "def specInfo(rec_file):\n",
    "    wave_info, framerate = wavInfo(rec_file)\n",
    "    spectrum, freqs, t, _ = pylab.specgram(wave_info, NFFT=512, noverlap=256, window=pylab.window_hanning, Fs=framerate)\n",
    "    del _\n",
    "    return spectrum, freqs, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search for the index of the leftmost value in an ordered array \n",
    "# (of times or frequencies in our case) that still meet our criteria\n",
    "def leftmostBinSearch(A, lo, hi, target):\n",
    "    mid = (lo + hi) // 2\n",
    "    v1 = A[mid]\n",
    "    if (v1 >= target):\n",
    "        if (mid > 0 and A[mid - 1] > target):\n",
    "            return leftmostBinSearch(A, lo, mid-1, target)\n",
    "        else:\n",
    "            return mid\n",
    "    elif (A[mid] < target):\n",
    "        return leftmostBinSearch(A, mid+1, hi, target)\n",
    "    else:\n",
    "        return leftmostBinSearch(A, lo, mid-1, target)\n",
    "\n",
    "# search for the index of the rightmost value in an ordered array \n",
    "# (of times or frequencies in our case) that still meet our criteria\n",
    "def rightmostBinSearch(A, lo, hi, target): # something is wrong and it's giving me 1 to the right \n",
    "    mid = (lo + hi) // 2\n",
    "    v1 = A[mid]\n",
    "    if (v1 <= target):\n",
    "        if (mid < (len(A) - 1) and A[mid + 1] <= target):\n",
    "            return rightmostBinSearch(A, mid+1, hi, target)\n",
    "        else:\n",
    "            return mid\n",
    "    elif (A[mid] < target):\n",
    "        return rightmostBinSearch(A, mid+1, hi, target)\n",
    "    else:\n",
    "        return rightmostBinSearch(A, lo, mid-1, target)\n",
    "    \n",
    "# Calls on rightmostBinSearch and leftmostBinSearch\n",
    "def getBounds(A, minVal, maxVal):\n",
    "    left = leftmostBinSearch(A, 0, len(A)-1, minVal)\n",
    "    right = rightmostBinSearch(A, 0, len(A)-1, maxVal)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def specMod(spectrum, freqs, times, f1, f2, t1, t2):\n",
    "    spectrumMod = [spectrum[f1][t1:t2]]\n",
    "    for f in range(f1+1, f2): # check when fix right limit\n",
    "            spectrumMod = spectrumMod + [spectrum[f][t1:t2]]\n",
    "    return spectrumMod\n",
    "\n",
    "# Plots the spectrogram\n",
    "def plotModSpecSimple(specMod, freqs, times, file):\n",
    "    fig, ax = pylab.subplots(1)\n",
    "    pylab.pcolormesh(times, freqs, 10 * pylab.log10(specMod))\n",
    "    fig.subplots_adjust(left=0,right=1,bottom=0,top=1)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    pylab.savefig(file)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the speciesData dictionary which is a dict with all the information we have regarding the different species. \n",
    "def speciesData(workbook):\n",
    "    roi_ws = openpyxl.load_workbook(workbook)['ROIs']\n",
    "    dataset = {}\n",
    "    # needed format:\n",
    "    # species specimen per row\n",
    "    # columns: species name, start_time, end_time, min_freq, max_freq, recording name\n",
    "    # columns A to F\n",
    "    sheetMatrix = list(roi_ws.iter_rows())\n",
    "    # remove row with column names and create array of keys per species. (e.g. start_time, end_time, ...)\n",
    "    keys = sheetMatrix.pop(0) \n",
    "    for row in sheetMatrix:\n",
    "        speciesName = row[0].value\n",
    "        if (speciesName not in dataset):\n",
    "            dataset[speciesName] = {}\n",
    "        for col in range(1,len(row)):\n",
    "            cell = ''\n",
    "            # change recording extension since we are dealing with wav files\n",
    "            if (col == 5):\n",
    "                cell = row[col].value\n",
    "                cell += '.wav'\n",
    "            else:\n",
    "                cell = row[col].value\n",
    "            # if per species key is not present add the key and add the value as the first element in a list\n",
    "            if (keys[col].value not in dataset[speciesName]): \n",
    "                dataset[speciesName][keys[col].value] = [cell]\n",
    "            # append to the list of attributes \n",
    "            else:\n",
    "                dataset[speciesName][keys[col].value] = dataset[speciesName][keys[col].value] + [cell]\n",
    "    return dataset\n",
    "\n",
    "# Convert speciesData dictionary to yaml and save file\n",
    "def dataToYAML(data, name): \n",
    "    # need to check if file exists then delete it\n",
    "    path = '../dataset/' + name\n",
    "    dataset = open(path, 'w+')\n",
    "    dump = yaml.dump(data, dataset, default_flow_style=False)\n",
    "    dataset.close()\n",
    "\n",
    "# As it names suggests, it find the maximum. \n",
    "def findMax(L):\n",
    "    Max = float('-inf')\n",
    "    for n in L:\n",
    "        if (n > Max):\n",
    "            Max = n\n",
    "    return Max\n",
    "\n",
    "# As it names suggests, it find the minimum. \n",
    "def findMin(L):\n",
    "    Min = float('inf')\n",
    "    for n in L:\n",
    "        if (n < Min):\n",
    "            Min = n\n",
    "    return Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simplifiedSpeciesData(data): \n",
    "    simplDat = {}\n",
    "    for species in data:\n",
    "        min_freqs = data[species]['min_frequency']\n",
    "        max_freqs = data[species]['max_frequency']\n",
    "        min_times = data[species]['start_time']\n",
    "        max_times = data[species]['end_time']\n",
    "        min_f = findMin(min_freqs)\n",
    "        max_f = findMax(max_freqs)\n",
    "        start = findMin(min_times)\n",
    "        end = findMax(max_times)\n",
    "        simplDat[species] = {'min_freq':min_f, 'max_freq':max_f, 'delta_time':(end - start), 'recording name':data[species]['recording name']}\n",
    "    return simplDat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that this data was manually labeled, and this is very time intensive, we have less validations than recordings, so we have to make sure to only be managing the recordings for which we have validations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loads the validation data\n",
    "df = pd.ExcelFile('../dataset/validationsAndROIs.xlsx')\n",
    "df = df.parse('ROIs')\n",
    "\n",
    "# Gets the name of all the rcordings\n",
    "all_wav_files = []\n",
    "for file in os.listdir(\"../dataset/wav_recordings\"):\n",
    "    all_wav_files.append(file[:file.index('.')])\n",
    "\n",
    "# Extracts the recording_name column and stores it as a list\n",
    "recording_name = df[\"recording name\"].tolist()\n",
    "\n",
    "# Formats the string containing the name of the recording to remove everything after the first dot.\n",
    "for i in range(len(recording_name)):\n",
    "    recording_name[i] = recording_name[i][:recording_name[i].index('.')]\n",
    "\n",
    "# If we don't have a recording, then the validation data is not useful by itself. So we want to remove this data. \n",
    "for i in range(len(recording_name)):\n",
    "    if recording_name[i] not in all_wav_files:\n",
    "        recording_name[i] = \"delete\"\n",
    "\n",
    "# Creates the column 'recording name' initialized with the corresponding values in tecording_name list, \n",
    "# then removes the data labeled as delete\n",
    "\n",
    "df['recording name'] = recording_name\n",
    "df = df[df['recording name'] != 'delete']\n",
    "\n",
    "# Writes out the corrected validation data. \n",
    "writer = pd.ExcelWriter('../dataset/corrected_validationsAndROIs.xlsx')\n",
    "df.to_excel(writer, 'ROIs', index=False)\n",
    "writer.save()\n",
    "\n",
    "# Standarizes the names of all the recording names\n",
    "for files in os.listdir(\"../dataset/wav_recordings\"):\n",
    "    new_name = files[:files.index('.')]\n",
    "    os.rename(\"../dataset/wav_recordings/\" + files, \"../dataset/wav_recordings/\" + new_name + '.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to export the different species recordings in pickle format, because it is easier to manipulate later on. But also want to export the dictionary in a .yaml file for later use. Also, we want a compressed version of this data in case it becomes necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save our species data dictionary as a .yaml file for later use\n",
    "workbook = '../dataset/corrected_validationsAndROIs.xlsx'\n",
    "data = speciesData(workbook)\n",
    "dataToYAML(data, 'dataset.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRawSpecDataset(dataset, path='../dataset'):\n",
    "    \n",
    "    # make directory to store our spec dataset\n",
    "    dataset_path = path + '/spectrogram_roi_dataset'\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "    else:\n",
    "        shutil.rmtree(dataset_path)\n",
    "        os.makedirs(dataset_path)\n",
    "    species = dataset.keys()\n",
    "    \n",
    "    # image data to be pickled \n",
    "    specs = []\n",
    "    \n",
    "    for s in species:\n",
    "        s_dir = dataset_path + '/' + s\n",
    "        s_spec = []\n",
    "        os.makedirs(s_dir) # make a directory per species\n",
    "        \n",
    "        # load species ROI data\n",
    "        min_freqs = dataset[s]['min_frequency']\n",
    "        max_freqs = dataset[s]['max_frequency']\n",
    "        starts = dataset[s]['start_time']\n",
    "        ends = dataset[s]['end_time']\n",
    "        recs = dataset[s]['recording name']\n",
    "        \n",
    "        for i in range(0, len(recs)):\n",
    "            rec = '../dataset/wav_recordings/' + recs[i] # path to ith recording file where s is present\n",
    "            spectrum, freqs, times = specInfo(rec) # get entire spectrogram data from rec\n",
    "            \n",
    "            # get ROI info in rec\n",
    "            t_0 = starts[i] \n",
    "            t_n = ends[i]\n",
    "            f_0 = min_freqs[i]\n",
    "            f_n = max_freqs[i]\n",
    "            \n",
    "            # find closest times and freqs that match ROI info\n",
    "            t_start, t_end = getBounds(times, t_0, t_n)\n",
    "            f_start, f_end = getBounds(freqs, f_0, f_n)\n",
    "            \n",
    "            # get modified spectrum, freqs, and times\n",
    "            spectrumMod = specMod(spectrum, freqs, times, f_start, f_end, t_start, t_end)\n",
    "            freqMod = freqs[f_start:f_end]\n",
    "            timeMod = times[t_start:t_end]\n",
    "            filename = s_dir + '/' + s + '_spec_' + str(i+1) + '.png'\n",
    "            \n",
    "            # plot the spectrogram of ROI and save the image \n",
    "            f = plotModSpecSimple(spectrumMod, freqMod, timeMod, filename)\n",
    "            s_spec.append(f) # append image to list of ROI spectrograms per species\n",
    "            pylab.close\n",
    "        \n",
    "        # add dictionary with key <species_name> and value <list_of_spectrogram_figures>\n",
    "        specs.append({s:s_spec}) \n",
    "    \n",
    "    return specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamlData = open('../dataset/dataset.yaml', 'r')\n",
    "dataset = yaml.load(yamlData)\n",
    "yamlData.close()\n",
    "data = getRawSpecDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickles the data\n",
    "def serializeDataset(obj, path='../dataset'):\n",
    "    pickle_path = path + '/pickle_data'\n",
    "    # create pickle directory if exists, else overwrite it\n",
    "    if not os.path.exists(pickle_path):\n",
    "        os.makedirs(pickle_path)\n",
    "    else:\n",
    "        shutil.rmtree(pickle_path)\n",
    "        os.makedirs(pickle_path)\n",
    "    for s in obj:\n",
    "        species = list(s.keys())[0]\n",
    "        data = s[species]\n",
    "        picklename = pickle_path + '/' + species + '.pickle'\n",
    "        with open(picklename, 'wb+') as pn:\n",
    "            cpl.dump(data, pn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "serializeDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compresses the data \n",
    "def archiveAndCompress(path):\n",
    "    directory = path.split('/')[0:-1]\n",
    "    directory = '/'.join(directory)\n",
    "    archive_name = directory + '/' + path.split('/')[-1] + '.tar.bz2'\n",
    "    with tarfile.open(archive_name, 'w:bz2') as archive:\n",
    "        folder = os.listdir(path)\n",
    "        for f in folder:\n",
    "            f = path + '/' + f\n",
    "            archive.add(f, arcname=os.path.basename(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "archiveAndCompress('../dataset/spectrogram_roi_dataset')\n",
    "archiveAndCompress('../dataset/pickle_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

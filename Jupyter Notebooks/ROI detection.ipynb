{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pylab\n",
    "import wave\n",
    "import openpyxl\n",
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "import _pickle as cpl\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the recordings and separate them by format. \n",
    "flac_files = []\n",
    "wav_files = []\n",
    "for file in os.listdir(\"../../dataset/recordings\"):\n",
    "    if file.endswith(\".flac\"):\n",
    "        flac_files.append(file)\n",
    "    elif file.endswith(\".wav\"):\n",
    "        wav_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the .flac files to .wav files and store them in 'wav_recordings'\n",
    "os.makedirs(\"../../dataset/wav_recordings\")\n",
    "for i in range(len(flac_files)):\n",
    "    string = 'sox ../../dataset/recordings/' + str(flac_files[i]) + ' ../../dataset/wav_recordings/' + str(flac_files[i][:-5]) + '.wav'\n",
    "    os.system(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the .wav files in 'wav_recordings'\n",
    "for i in range(len(wav_files)):\n",
    "    string = 'mv ../../dataset/' + str(wav_files[i]) + ' ../../dataset/wav_recordings'\n",
    "    os.system(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the validation data\n",
    "df = pd.ExcelFile('../../dataset/validationsAndROIs.xlsx')\n",
    "df = df.parse('ROIs')\n",
    "rs =[]\n",
    "\n",
    "\n",
    "# Gets the name of all the rcordings\n",
    "all_wav_files = []\n",
    "for file in os.listdir(\"../../dataset/wav_recordings\"):\n",
    "    all_wav_files.append(file[:file.index('.')])\n",
    "\n",
    "# Extracts the recording_name column and stores it as a list\n",
    "recording_name = df[\"recording name\"].tolist()\n",
    "\n",
    "# Formats the string containing the name of the recording to remove everything after the first dot.\n",
    "for i in range(len(recording_name)):\n",
    "    recording_name[i] = recording_name[i][:recording_name[i].index('.')]\n",
    "\n",
    "# If we don't have a recording, then the validation data is not useful by itself. So we want to remove this data. \n",
    "for i in range(len(recording_name)):\n",
    "    if recording_name[i] not in all_wav_files:\n",
    "        rs.append(recording_name[i])\n",
    "        recording_name[i] = \"delete\"\n",
    "\n",
    "# Creates the column 'recording name' initialized with the corresponding values in tecording_name list, \n",
    "# then removes the data labeled as delete\n",
    "\n",
    "df['recording name'] = recording_name\n",
    "df = df[df['recording name'] != 'delete']\n",
    "\n",
    "# Writes out the corrected validation data. \n",
    "writer = pd.ExcelWriter('../../dataset/corrected_validationsAndROIs.xlsx')\n",
    "df.to_excel(writer, 'ROIs', index=False)\n",
    "writer.save()\n",
    "\n",
    "# Standarizes the names of all the recording names\n",
    "for files in os.listdir(\"../../dataset/wav_recordings\"):\n",
    "    new_name = files[:files.index('.')]\n",
    "    os.rename(\"../../dataset/wav_recordings/\" + files, \"../../dataset/wav_recordings/\" + new_name + '.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grabaciones = list(df['recording name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grabaciones = [grabacion + '.wav' for grabacion in grabaciones]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Info from the .wav file.\n",
    "def wavInfo(rec_file):\n",
    "    wav_file = wave.open(rec_file, 'r')\n",
    "    frames = wav_file.readframes(-1)\n",
    "    wave_info = pylab.fromstring(frames, 'Int16') #all .wavs in our dataset are 16bit\n",
    "    framerate = wav_file.getframerate()\n",
    "    wav_file.close()\n",
    "    return wave_info, framerate\n",
    "\n",
    "# Get the info from the Spectrogram, but don't plot it.\n",
    "def specInfo(rec_file_list):\n",
    "    for rec_file in rec_file_list:\n",
    "        new_file = '../../dataset/spectrograms/' + rec_file + '.png'\n",
    "        rec_file = '../../dataset/wav_recordings/'+rec_file\n",
    "        wave_info, framerate = wavInfo(rec_file)\n",
    "        spectrum, freqs, t, _ = pylab.specgram(wave_info, \n",
    "                                               NFFT=512, \n",
    "                                               noverlap=256, \n",
    "                                               window=pylab.window_hanning, \n",
    "                                               Fs=framerate)\n",
    "        pylab.axis('off')\n",
    "        pylab.savefig(new_file,\n",
    "                      bbox_inches='tight', transparent = True)\n",
    "        pylab.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"../../dataset/spectrograms\")\n",
    "spectrum = specInfo(grabaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the speciesData dictionary which is a dict with all the information we have regarding the different species. \n",
    "def speciesData(workbook):\n",
    "    roi_ws = openpyxl.load_workbook(workbook)['ROIs']\n",
    "    dataset = {}\n",
    "    # needed format:\n",
    "    # species specimen per row\n",
    "    # columns: species name, start_time, end_time, min_freq, max_freq, recording name\n",
    "    # columns A to F\n",
    "    sheetMatrix = list(roi_ws.iter_rows())\n",
    "    # remove row with column names and create array of keys per species. (e.g. start_time, end_time, ...)\n",
    "    keys = sheetMatrix.pop(0) \n",
    "    for row in sheetMatrix:\n",
    "        speciesName = row[0].value\n",
    "        if (speciesName not in dataset):\n",
    "            dataset[speciesName] = {}\n",
    "        for col in range(1,len(row)):\n",
    "            cell = ''\n",
    "            # change recording extension since we are dealing with wav files\n",
    "            if (col == 5):\n",
    "                cell = row[col].value\n",
    "                cell += '.wav'\n",
    "            else:\n",
    "                cell = row[col].value\n",
    "            # if per species key is not present add the key and add the value as the first element in a list\n",
    "            if (keys[col].value not in dataset[speciesName]): \n",
    "                dataset[speciesName][keys[col].value] = [cell]\n",
    "            # append to the list of attributes \n",
    "            else:\n",
    "                dataset[speciesName][keys[col].value] = dataset[speciesName][keys[col].value] + [cell]\n",
    "    return dataset\n",
    "\n",
    "# Convert speciesData dictionary to yaml and save file\n",
    "def dataToYAML(data, name): \n",
    "    # need to check if file exists then delete it\n",
    "    path = name\n",
    "    dataset = open(path, 'w+')\n",
    "    dump = yaml.dump(data, dataset, default_flow_style=False)\n",
    "    dataset.close()\n",
    "\n",
    "# As it names suggests, it find the maximum. \n",
    "def findMax(L):\n",
    "    Max = float('-inf')\n",
    "    for n in L:\n",
    "        if (n > Max):\n",
    "            Max = n\n",
    "    return Max\n",
    "\n",
    "# As it names suggests, it find the minimum. \n",
    "def findMin(L):\n",
    "    Min = float('inf')\n",
    "    for n in L:\n",
    "        if (n < Min):\n",
    "            Min = n\n",
    "    return Min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our species data dictionary as a .yaml file for later use\n",
    "workbook = '../../dataset/corrected_validationsAndROIs.xlsx'\n",
    "data = speciesData(workbook)\n",
    "dataToYAML(data, '../../dataset/dataset.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamlData = open('../../dataset/dataset.yaml', 'r')\n",
    "dataset = yaml.load(yamlData)\n",
    "yamlData.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the index of the leftmost value in an ordered array \n",
    "# (of times or frequencies in our case) that still meet our criteria\n",
    "def leftmostBinSearch(A, lo, hi, target):\n",
    "    mid = (lo + hi) // 2\n",
    "    v1 = A[mid]\n",
    "    if (v1 >= target):\n",
    "        if (mid > 0 and A[mid - 1] > target):\n",
    "            return leftmostBinSearch(A, lo, mid-1, target)\n",
    "        else:\n",
    "            return mid\n",
    "    elif (A[mid] < target):\n",
    "        return leftmostBinSearch(A, mid+1, hi, target)\n",
    "    else:\n",
    "        return leftmostBinSearch(A, lo, mid-1, target)\n",
    "\n",
    "# search for the index of the rightmost value in an ordered array \n",
    "# (of times or frequencies in our case) that still meet our criteria\n",
    "def rightmostBinSearch(A, lo, hi, target): # something is wrong and it's giving me 1 to the right \n",
    "    mid = (lo + hi) // 2\n",
    "    v1 = A[mid]\n",
    "    if (v1 <= target):\n",
    "        if (mid < (len(A) - 1) and A[mid + 1] <= target):\n",
    "            return rightmostBinSearch(A, mid+1, hi, target)\n",
    "        else:\n",
    "            return mid\n",
    "    elif (A[mid] < target):\n",
    "        return rightmostBinSearch(A, mid+1, hi, target)\n",
    "    else:\n",
    "        return rightmostBinSearch(A, lo, mid-1, target)\n",
    "    \n",
    "# Calls on rightmostBinSearch and leftmostBinSearch\n",
    "def getBounds(A, minVal, maxVal):\n",
    "    left = leftmostBinSearch(A, 0, len(A)-1, minVal)\n",
    "    right = rightmostBinSearch(A, 0, len(A)-1, maxVal)\n",
    "    return left, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "x = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRawSpecDataset(dataset, path='../../dataset'):\n",
    "    \n",
    "    # make directory to store our spec dataset\n",
    "    dataset_path = path + '/spectrogram_roi_dataset'\n",
    "    if not os.path.exists(dataset_path):\n",
    "        os.makedirs(dataset_path)\n",
    "    else:\n",
    "        shutil.rmtree(dataset_path)\n",
    "        os.makedirs(dataset_path)\n",
    "    species = dataset.keys()\n",
    "    \n",
    "    # image data to be pickled \n",
    "    specs = []\n",
    "    \n",
    "    for s in species:\n",
    "        #s_dir = dataset_path + '/' + s\n",
    "        s_spec = []\n",
    "        #os.makedirs(s_dir) # make a directory per species\n",
    "        \n",
    "        # load species ROI data\n",
    "        min_freqs = dataset[s]['min_frequency']\n",
    "        max_freqs = dataset[s]['max_frequency']\n",
    "        starts = dataset[s]['start_time']\n",
    "        ends = dataset[s]['end_time']\n",
    "        recs = dataset[s]['recording name']\n",
    "        print(s)\n",
    "        \n",
    "        for i in range(0, len(recs)):\n",
    "            \n",
    "            rec = '../../dataset/wav_recordings/' + recs[i] # path to ith recording file where s is present\n",
    "            \n",
    "            wave_info, framerate = wavInfo(rec)\n",
    "            spectrum, freqs, times, _ = pylab.specgram(wave_info, \n",
    "                                               NFFT=512, \n",
    "                                               noverlap=256, \n",
    "                                               window=pylab.window_hanning, \n",
    "                                               Fs=framerate)\n",
    "            freqs = np.asarray(freqs)\n",
    "            #freqs = freqs.flatten(freqs)\n",
    "            \n",
    "            x.append(freqs)\n",
    "            pylab.axis('off')\n",
    "            pylab.close()\n",
    "            \n",
    "            #spectrum, freqs, times = specInfo(rec) # get entire spectrogram data from rec\n",
    "            \n",
    "            # get ROI info in rec\n",
    "            t_0 = starts[i] \n",
    "            t_n = ends[i]\n",
    "            f_0 = min_freqs[i]\n",
    "            f_n = max_freqs[i]\n",
    "            \n",
    "            \n",
    "            # find closest times and freqs that match ROI info\n",
    "            t_start, t_end = getBounds(times, t_0, t_n)\n",
    "            f_start, f_end = getBounds(freqs, f_0, f_n)\n",
    "            \n",
    "            '''\n",
    "            # get modified spectrum, freqs, and times\n",
    "            #spectrumMod = specMod(spectrum, freqs, times, f_start, f_end, t_start, t_end)\n",
    "            freqMod = freqs[f_start:f_end]\n",
    "            timeMod = times[t_start:t_end]\n",
    "            \n",
    "            '''\n",
    "            y.append([t_0, f_0, t_n, f_n])\n",
    "            \n",
    "            #np.array([t_start, f_start, t_end, f_end])\n",
    "            \n",
    "    #return np.array([t_0, f_0, t_n, f_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basileuterus bivittatus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Numeric-style type codes are deprecated and will result in an error in the future.\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basileuterus chrysogaster\n",
      "Chlorothraupis carmioli\n",
      "Eleutherodactylus brittoni\n",
      "Eleutherodactylus cochranae\n",
      "Eleutherodactylus cooki\n",
      "Eleutherodactylus coqui\n",
      "Eleutherodactylus juanariveroi\n",
      "Epinephelus guttatus\n",
      "Formicarius analis\n",
      "Hypocnemis subflava\n",
      "Liosceles thoracicus\n",
      "Megascops guatemalae\n",
      "Megascops nudipes\n",
      "Microcerculus marginatus\n",
      "Myrmeciza hemimelaena\n",
      "Myrmoborus leucophrys\n",
      "Percnostola lophotes\n",
      "Saltator grossus\n",
      "Thamnophilus schistaceus\n",
      "Unknown Insect\n"
     ]
    }
   ],
   "source": [
    "data = getRawSpecDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40.7895215572, 1805.81896552, 43.1253410951, 5341.42241379]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0.       ,    86.1328125,   172.265625 ,   258.3984375,\n",
       "         344.53125  ,   430.6640625,   516.796875 ,   602.9296875,\n",
       "         689.0625   ,   775.1953125,   861.328125 ,   947.4609375,\n",
       "        1033.59375  ,  1119.7265625,  1205.859375 ,  1291.9921875,\n",
       "        1378.125    ,  1464.2578125,  1550.390625 ,  1636.5234375,\n",
       "        1722.65625  ,  1808.7890625,  1894.921875 ,  1981.0546875,\n",
       "        2067.1875   ,  2153.3203125,  2239.453125 ,  2325.5859375,\n",
       "        2411.71875  ,  2497.8515625,  2583.984375 ,  2670.1171875,\n",
       "        2756.25     ,  2842.3828125,  2928.515625 ,  3014.6484375,\n",
       "        3100.78125  ,  3186.9140625,  3273.046875 ,  3359.1796875,\n",
       "        3445.3125   ,  3531.4453125,  3617.578125 ,  3703.7109375,\n",
       "        3789.84375  ,  3875.9765625,  3962.109375 ,  4048.2421875,\n",
       "        4134.375    ,  4220.5078125,  4306.640625 ,  4392.7734375,\n",
       "        4478.90625  ,  4565.0390625,  4651.171875 ,  4737.3046875,\n",
       "        4823.4375   ,  4909.5703125,  4995.703125 ,  5081.8359375,\n",
       "        5167.96875  ,  5254.1015625,  5340.234375 ,  5426.3671875,\n",
       "        5512.5      ,  5598.6328125,  5684.765625 ,  5770.8984375,\n",
       "        5857.03125  ,  5943.1640625,  6029.296875 ,  6115.4296875,\n",
       "        6201.5625   ,  6287.6953125,  6373.828125 ,  6459.9609375,\n",
       "        6546.09375  ,  6632.2265625,  6718.359375 ,  6804.4921875,\n",
       "        6890.625    ,  6976.7578125,  7062.890625 ,  7149.0234375,\n",
       "        7235.15625  ,  7321.2890625,  7407.421875 ,  7493.5546875,\n",
       "        7579.6875   ,  7665.8203125,  7751.953125 ,  7838.0859375,\n",
       "        7924.21875  ,  8010.3515625,  8096.484375 ,  8182.6171875,\n",
       "        8268.75     ,  8354.8828125,  8441.015625 ,  8527.1484375,\n",
       "        8613.28125  ,  8699.4140625,  8785.546875 ,  8871.6796875,\n",
       "        8957.8125   ,  9043.9453125,  9130.078125 ,  9216.2109375,\n",
       "        9302.34375  ,  9388.4765625,  9474.609375 ,  9560.7421875,\n",
       "        9646.875    ,  9733.0078125,  9819.140625 ,  9905.2734375,\n",
       "        9991.40625  , 10077.5390625, 10163.671875 , 10249.8046875,\n",
       "       10335.9375   , 10422.0703125, 10508.203125 , 10594.3359375,\n",
       "       10680.46875  , 10766.6015625, 10852.734375 , 10938.8671875,\n",
       "       11025.       , 11111.1328125, 11197.265625 , 11283.3984375,\n",
       "       11369.53125  , 11455.6640625, 11541.796875 , 11627.9296875,\n",
       "       11714.0625   , 11800.1953125, 11886.328125 , 11972.4609375,\n",
       "       12058.59375  , 12144.7265625, 12230.859375 , 12316.9921875,\n",
       "       12403.125    , 12489.2578125, 12575.390625 , 12661.5234375,\n",
       "       12747.65625  , 12833.7890625, 12919.921875 , 13006.0546875,\n",
       "       13092.1875   , 13178.3203125, 13264.453125 , 13350.5859375,\n",
       "       13436.71875  , 13522.8515625, 13608.984375 , 13695.1171875,\n",
       "       13781.25     , 13867.3828125, 13953.515625 , 14039.6484375,\n",
       "       14125.78125  , 14211.9140625, 14298.046875 , 14384.1796875,\n",
       "       14470.3125   , 14556.4453125, 14642.578125 , 14728.7109375,\n",
       "       14814.84375  , 14900.9765625, 14987.109375 , 15073.2421875,\n",
       "       15159.375    , 15245.5078125, 15331.640625 , 15417.7734375,\n",
       "       15503.90625  , 15590.0390625, 15676.171875 , 15762.3046875,\n",
       "       15848.4375   , 15934.5703125, 16020.703125 , 16106.8359375,\n",
       "       16192.96875  , 16279.1015625, 16365.234375 , 16451.3671875,\n",
       "       16537.5      , 16623.6328125, 16709.765625 , 16795.8984375,\n",
       "       16882.03125  , 16968.1640625, 17054.296875 , 17140.4296875,\n",
       "       17226.5625   , 17312.6953125, 17398.828125 , 17484.9609375,\n",
       "       17571.09375  , 17657.2265625, 17743.359375 , 17829.4921875,\n",
       "       17915.625    , 18001.7578125, 18087.890625 , 18174.0234375,\n",
       "       18260.15625  , 18346.2890625, 18432.421875 , 18518.5546875,\n",
       "       18604.6875   , 18690.8203125, 18776.953125 , 18863.0859375,\n",
       "       18949.21875  , 19035.3515625, 19121.484375 , 19207.6171875,\n",
       "       19293.75     , 19379.8828125, 19466.015625 , 19552.1484375,\n",
       "       19638.28125  , 19724.4140625, 19810.546875 , 19896.6796875,\n",
       "       19982.8125   , 20068.9453125, 20155.078125 , 20241.2109375,\n",
       "       20327.34375  , 20413.4765625, 20499.609375 , 20585.7421875,\n",
       "       20671.875    , 20758.0078125, 20844.140625 , 20930.2734375,\n",
       "       21016.40625  , 21102.5390625, 21188.671875 , 21274.8046875,\n",
       "       21360.9375   , 21447.0703125, 21533.203125 , 21619.3359375,\n",
       "       21705.46875  , 21791.6015625, 21877.734375 , 21963.8671875,\n",
       "       22050.       ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray(x)\n",
    "bbox = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, bbox, test_size=0.33, random_state=42)\n",
    "\n",
    "#X_train = np.asarray(X_train)\n",
    "#y_train = np.asarray(y_train)\n",
    "\n",
    "#X_test = np.asarray(X_test)\n",
    "#y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "model = Sequential([\n",
    "        Dense(4096, input_dim=X_train.shape[-1]),\n",
    "        Dense(4096),\n",
    "        Dense(4096),\n",
    "        Dense(4096),\n",
    "        Dense(4096),\n",
    "        Activation('relu'), \n",
    "        Dropout(0.2), \n",
    "        Dense(y_train.shape[-1])\n",
    "    ])\n",
    "model.compile('adadelta', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 213 samples, validate on 106 samples\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, nb_epoch=30, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
